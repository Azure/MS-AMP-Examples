diff --git a/megatron/arguments.py b/megatron/arguments.py
index bdd1745..4058c11 100644
--- a/megatron/arguments.py
+++ b/megatron/arguments.py
@@ -46,6 +46,7 @@ def parse_args(extra_args_provider=None, defaults={},
     parser = _add_memoryopt_args(parser)
     parser = _add_activation_checkpoint_args(parser)
     parser = _add_distillation_args(parser)
+    parser = _add_msamp_args(parser)
 
     # Custom arguments.
     if extra_args_provider is not None:
@@ -936,3 +937,11 @@ def _add_distillation_args(parser):
                        help='Directory containing a teacher model checkpoint.')
 
     return parser
+
+
+def _add_msamp_args(parser):
+    group = parser.add_argument_group('MS-AMP', 'MS-AMP configurations')
+    group.add_argument('--msamp', action='store_true', help='Enable MS-AMP', default=False)
+    group.add_argument('--msamp-opt-level', type=str, default='O2', choices=['O1', 'O2', 'O3'],
+                       help='MS-AMP optimization level')
+    return parser
diff --git a/megatron/fused_kernels/layer_norm_cuda_kernel.cu b/megatron/fused_kernels/layer_norm_cuda_kernel.cu
index 8a07806..7336f45 100644
--- a/megatron/fused_kernels/layer_norm_cuda_kernel.cu
+++ b/megatron/fused_kernels/layer_norm_cuda_kernel.cu
@@ -246,21 +246,21 @@ void cuWelfordMuSigma2(
     }
   }
 }
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
 template<typename U> U rsqrt(U v) {
 #else
 template<typename U> __device__ U rsqrt(U v) {
 #endif
   return U(1) / sqrt(v);
 }
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
 template<> float rsqrt(float v) {
 #else
 template<> __device__ float rsqrt(float v) {
 #endif
   return rsqrtf(v);
 }
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
 template<> double rsqrt(double v) {
 #else
 template<> __device__ double rsqrt(double v) {
@@ -316,7 +316,7 @@ void cuApplyLayerNorm(
   // 1) blockDim.x == warpSize
   // 2) Tensors are contiguous
   //
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
   for (auto i1=blockIdx.y; i1 < n1; i1 += gridDim.y) {
 #else
   for (int i1=blockIdx.y; i1 < n1; i1 += gridDim.y) {
@@ -559,7 +559,7 @@ void cuComputeGradInput(
     const V* gamma,
     T* grad_input)
 {
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
   for (auto i1=blockIdx.y; i1 < n1; i1 += gridDim.y) {
 #else
   for (int i1=blockIdx.y; i1 < n1; i1 += gridDim.y) {
@@ -689,7 +689,7 @@ void HostApplyLayerNorm(
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     const int warp_size = at::cuda::warp_size();
     dim3 threads(warp_size,4,1);
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
     threads.y = 1;
 #endif
     const uint64_t maxGridY =
@@ -765,7 +765,7 @@ void HostLayerNormGradient(
 
     if (gamma != NULL && beta != NULL) {
       // compute grad_gamma(j) and grad_beta(j)
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
       const int part_size = warp_size;
 #else
       const int part_size = 16;
@@ -806,7 +806,7 @@ void HostLayerNormGradient(
       at::cuda::getCurrentDeviceProperties()->maxGridSize[1];
     const dim3 blocks1(1, std::min((uint64_t)n1, maxGridY), 1);
     dim3 threads1(warp_size,4,1);
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
     threads1.y = 2;
 #endif
     int nshared =
diff --git a/megatron/fused_kernels/scaled_masked_softmax_cuda.cu b/megatron/fused_kernels/scaled_masked_softmax_cuda.cu
index c034dc3..37613ee 100644
--- a/megatron/fused_kernels/scaled_masked_softmax_cuda.cu
+++ b/megatron/fused_kernels/scaled_masked_softmax_cuda.cu
@@ -18,7 +18,7 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <cuda_fp16.h>
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
 #include <cuda_profiler_api.h>
 #endif
 #include <ATen/cuda/CUDAContext.h>
diff --git a/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu b/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu
index 59e4525..c99c743 100644
--- a/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu
+++ b/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu
@@ -18,7 +18,7 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <cuda_fp16.h>
-#ifndef __HIP_PLATFORM_HCC__
+#ifndef __HIP_PLATFORM_AMD__
 #include <cuda_profiler_api.h>
 #endif
 #include <ATen/cuda/CUDAContext.h>
diff --git a/megatron/mpu/__init__.py b/megatron/mpu/__init__.py
index 32bb5fc..dd894a4 100644
--- a/megatron/mpu/__init__.py
+++ b/megatron/mpu/__init__.py
@@ -23,6 +23,7 @@ from .initialize import is_unitialized
 from .initialize import destroy_model_parallel
 from .initialize import get_data_parallel_group
 from .initialize import get_data_parallel_rank
+from .initialize import get_data_parallel_src_rank
 from .initialize import get_data_parallel_world_size
 from .initialize import get_embedding_group
 from .initialize import get_model_parallel_group
diff --git a/megatron/mpu/initialize.py b/megatron/mpu/initialize.py
index c24d117..dda8269 100644
--- a/megatron/mpu/initialize.py
+++ b/megatron/mpu/initialize.py
@@ -45,6 +45,10 @@ _MPU_PIPELINE_MODEL_PARALLEL_RANK = None
 # rank when broadcasting from the first or last pipeline stage
 _PIPELINE_GLOBAL_RANKS = None
 
+# A list of global ranks for each data parallel group to ease calculation of the source
+# rank when broadcasting weights from src to all other data parallel ranks
+_DATA_PARALLEL_GLOBAL_RANKS = None
+
 def is_unitialized():
     """Useful for code segments that may be accessed with or without mpu initialization"""
     return _DATA_PARALLEL_GROUP is None
@@ -105,8 +109,13 @@ def initialize_model_parallel(tensor_model_parallel_size_=1,
 
     # Build the data-parallel groups.
     global _DATA_PARALLEL_GROUP
+    global _DATA_PARALLEL_GLOBAL_RANKS
+
     assert _DATA_PARALLEL_GROUP is None, \
         'data parallel group is already initialized'
+    assert _DATA_PARALLEL_GLOBAL_RANKS is None, \
+        'data parallel global ranks is already initialized'
+
     all_data_parallel_group_ranks = []
     for i in range(pipeline_model_parallel_size):
         start_rank = i * num_pipeline_model_parallel_groups
@@ -118,6 +127,7 @@ def initialize_model_parallel(tensor_model_parallel_size_=1,
             group = torch.distributed.new_group(ranks)
             if rank in ranks:
                 _DATA_PARALLEL_GROUP = group
+                _DATA_PARALLEL_GLOBAL_RANKS = ranks
 
     # Build the model-parallel groups.
     global _MODEL_PARALLEL_GROUP
@@ -198,6 +208,14 @@ def get_pipeline_model_parallel_group():
     return _PIPELINE_MODEL_PARALLEL_GROUP
 
 
+def get_data_parallel_src_rank():
+    """Calculate the global rank corresponding to the first local rank
+    in the data parallel group."""
+    assert _DATA_PARALLEL_GLOBAL_RANKS is not None, \
+        "Data parallel group is not initialized"
+    return _DATA_PARALLEL_GLOBAL_RANKS[0]
+
+
 def get_data_parallel_group():
     """Get the data parallel group the caller rank belongs to."""
     assert _DATA_PARALLEL_GROUP is not None, \
@@ -363,7 +381,6 @@ def get_data_parallel_rank():
     """Return my rank for the data parallel group."""
     return torch.distributed.get_rank(group=get_data_parallel_group())
 
-
 def destroy_model_parallel():
     """Set the groups to none."""
     global _TENSOR_MODEL_PARALLEL_GROUP
diff --git a/megatron/mpu/layers.py b/megatron/mpu/layers.py
index 5d168c6..3554270 100644
--- a/megatron/mpu/layers.py
+++ b/megatron/mpu/layers.py
@@ -21,6 +21,7 @@
 import math
 
 import torch
+import torch.nn as nn
 import torch.nn.functional as F
 import torch.nn.init as init
 from torch.nn.parameter import Parameter
@@ -254,36 +255,44 @@ class ColumnParallelLinear(torch.nn.Module):
         # Initialize weight.
         args = get_args()
         if args.use_cpu_initialization:
-            self.weight = Parameter(torch.empty(self.output_size_per_partition,
+            _weight = Parameter(torch.empty(self.output_size_per_partition,
                                                 self.input_size,
                                                 dtype=args.params_dtype))
             self.master_weight = _initialize_affine_weight_cpu(
-                self.weight, self.output_size, self.input_size,
+                _weight, self.output_size, self.input_size,
                 self.output_size_per_partition, 0, init_method,
                 stride=stride, return_master_weight=keep_master_weight_for_test)
         else:
-            self.weight = Parameter(torch.empty(
+            _weight = Parameter(torch.empty(
                 self.output_size_per_partition, self.input_size,
                 device=get_accelerator().current_device_name(), dtype=args.params_dtype))
-            _initialize_affine_weight_gpu(self.weight, init_method,
+            _initialize_affine_weight_gpu(_weight, init_method,
                                           partition_dim=0, stride=stride)
             
         if bias:
             if args.use_cpu_initialization:
-                self.bias = Parameter(torch.empty(
+                _bias = Parameter(torch.empty(
                     self.output_size_per_partition, dtype=args.params_dtype))
             else:
-                self.bias = Parameter(torch.empty(
+                _bias = Parameter(torch.empty(
                     self.output_size_per_partition,
                     device=get_accelerator().current_device_name(),
                     dtype=args.params_dtype))
-            set_tensor_model_parallel_attributes(self.bias, True, 0, stride)
+            set_tensor_model_parallel_attributes(_bias, True, 0, stride)
             # Always initialize bias to zero.
             with torch.no_grad():
-                self.bias.zero_()
+                _bias.zero_()
         else:
             self.register_parameter('bias', None)
 
+        self.linear = nn.Linear(out_features=self.output_size_per_partition, in_features=self.input_size,
+                                bias=not self.skip_bias_add, dtype=args.params_dtype)
+        self.linear.weight = _weight
+        if not self.skip_bias_add:
+            self.linear.bias = _bias
+        else:
+            self.output_bias = _bias
+
 
 
     def forward(self, input_):
@@ -294,15 +303,14 @@ class ColumnParallelLinear(torch.nn.Module):
             input_parallel = copy_to_tensor_model_parallel_region(input_)
 
         # Matrix multiply.
+        output_parallel = self.linear(input_parallel)
 
-        bias = self.bias if not self.skip_bias_add else None
-        output_parallel = F.linear(input_parallel, self.weight, bias)
         if self.gather_output and not self.is_expert_without_slicing:
             # All-gather across the partitions.
             output = gather_from_tensor_model_parallel_region(output_parallel)
         else:
-            output = output_parallel 
-        output_bias = self.bias if self.skip_bias_add else None
+            output = output_parallel
+        output_bias = self.output_bias if self.skip_bias_add else None
         return output, output_bias
 
 
@@ -365,18 +373,18 @@ class RowParallelLinear(torch.nn.Module):
         # Initialize weight.
         args = get_args()
         if args.use_cpu_initialization:
-            self.weight = Parameter(torch.empty(self.output_size,
+            _weight = Parameter(torch.empty(self.output_size,
                                                 self.input_size_per_partition,
                                                 dtype=args.params_dtype))
             self.master_weight = _initialize_affine_weight_cpu(
-                self.weight, self.output_size, self.input_size,
+                _weight, self.output_size, self.input_size,
                 self.input_size_per_partition, 1, init_method,
                 stride=stride, return_master_weight=keep_master_weight_for_test)
         else:
-            self.weight = Parameter(torch.empty(
+            _weight = Parameter(torch.empty(
                 self.output_size, self.input_size_per_partition,
                 device=get_accelerator().current_device_name(), dtype=args.params_dtype))
-            _initialize_affine_weight_gpu(self.weight, init_method,
+            _initialize_affine_weight_gpu(_weight, init_method,
                                           partition_dim=1, stride=stride)
         if bias:
             if args.use_cpu_initialization:
@@ -392,6 +400,10 @@ class RowParallelLinear(torch.nn.Module):
         else:
             self.register_parameter('bias', None)
 
+        assert skip_bias_add
+        self.linear = nn.Linear(out_features=self.output_size, in_features=self.input_size_per_partition,
+                                bias=False, dtype=args.params_dtype)
+        self.linear.weight = _weight
 
 
     def forward(self, input_):
@@ -401,7 +413,8 @@ class RowParallelLinear(torch.nn.Module):
         else:
             input_parallel = scatter_to_tensor_model_parallel_region(input_)
         # Matrix multiply.
-        output_parallel = F.linear(input_parallel, self.weight)
+        output_parallel = self.linear(input_parallel)
+
         # All-reduce across all the partitions.
         if self.is_expert_without_slicing: # non-expert only tensor-parallelism
             output_ = output_parallel
diff --git a/megatron/optimizer/__init__.py b/megatron/optimizer/__init__.py
index 659d680..19fda6c 100644
--- a/megatron/optimizer/__init__.py
+++ b/megatron/optimizer/__init__.py
@@ -12,6 +12,7 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import torch
 from deepspeed.accelerator import get_accelerator
 if get_accelerator().device_name() == 'cuda':
     from apex.optimizers import FusedAdam as Adam
@@ -20,6 +21,7 @@ else:
     from torch.optim import Adam
     from torch.optim import SGD
 
+from msamp.optim import LBAdamW
 
 from megatron import get_args
 from megatron.model import LayerNorm
@@ -59,7 +61,7 @@ def get_megatron_optimizer(model):
     if args.create_moe_param_group:
         from deepspeed.moe.utils import is_moe_param, split_params_into_different_moe_groups_for_optimizer
         param_groups = split_params_into_different_moe_groups_for_optimizer(param_groups)
-    
+
     if args.cpu_optimizer:
         assert args.optimizer == 'adam', 'CPU offloading is for Adam'
         if args.cpu_torch_adam:
@@ -71,20 +73,40 @@ def get_megatron_optimizer(model):
                                        lr=args.lr,
                                        weight_decay=args.weight_decay)
     else:
-        if args.optimizer == 'adam':
-            optimizer = Adam(param_groups,
-                            lr=args.lr,
-                            weight_decay=args.weight_decay,
-                            betas=(args.adam_beta1, args.adam_beta2),
-                            eps=args.adam_eps)
-        elif args.optimizer == 'sgd':
-            optimizer = SGD(param_groups,
-                            lr=args.lr,
-                            weight_decay=args.weight_decay,
-                            momentum=args.sgd_momentum)
+        if args.msamp:
+            print(f"Using MS-AMP optimizer, opt_level is {args.msamp_opt_level}")
+            if args.msamp_opt_level == 'O2' or args.msamp_opt_level == 'O3':
+                exp_avg_dtype = torch.uint8
+                exp_avg_sq_dtype = torch.float16
+            elif args.msamp_opt_level == 'O1':
+                exp_avg_dtype = torch.float32
+                exp_avg_sq_dtype = torch.float32
+            else:
+                raise Exception(f'Unsupported msamp_opt_level: {args.msamp_opt_level}')
+            optimizer = LBAdamW(param_groups,
+                                lr=args.lr,
+                                weight_decay=args.weight_decay,
+                                betas=(args.adam_beta1, args.adam_beta2),
+                                eps=args.adam_eps,
+                                exp_avg_dtype=exp_avg_dtype,
+                                exp_avg_sq_dtype=exp_avg_sq_dtype,
+                                tensor_scale=True,
+                                )
         else:
-            raise Exception('{} optimizer is not supported.'.format(
-            args.optimizer))
+            if args.optimizer == 'adam':
+                optimizer = Adam(param_groups,
+                                lr=args.lr,
+                                weight_decay=args.weight_decay,
+                                betas=(args.adam_beta1, args.adam_beta2),
+                                eps=args.adam_eps)
+            elif args.optimizer == 'sgd':
+                optimizer = SGD(param_groups,
+                                lr=args.lr,
+                                weight_decay=args.weight_decay,
+                                momentum=args.sgd_momentum)
+            else:
+                raise Exception('{} optimizer is not supported.'.format(
+                args.optimizer))
 
     if args.deepspeed:
         return optimizer
diff --git a/megatron/training.py b/megatron/training.py
index 94133e7..7100f03 100644
--- a/megatron/training.py
+++ b/megatron/training.py
@@ -60,6 +60,9 @@ from deepspeed.compression.compress import init_compression, redundancy_clean
 from megatron.model.transformer import  ParallelTransformerLayer
 from deepspeed.runtime.data_pipeline.data_routing.helper import convert_to_random_ltd
 
+from msamp import deepspeed
+from msamp.nn import LinearReplacer
+
 def print_datetime(string):
     """Note that this call will sync across all ranks."""
     torch.distributed.barrier()
@@ -436,6 +439,14 @@ def setup_model_and_optimizer(model_provider_func, teacher=False,
 
     model = get_model(model_provider_func)
 
+    if args.msamp:
+        assert len(model) == 1
+        model[0] = LinearReplacer.replace(model[0],
+                src_rank=mpu.get_data_parallel_src_rank(),
+                group=mpu.get_data_parallel_group())
+        print('after replaced with FP8Linear, model is: ')
+        print(model[0])
+
     # initialize the compression here
     student_global_steps = 0
     if args.kd or args.mos:
