diff --git a/megatron/arguments.py b/megatron/arguments.py
index ae42b83e..f427bc50 100644
--- a/megatron/arguments.py
+++ b/megatron/arguments.py
@@ -38,6 +38,7 @@ def parse_args(extra_args_provider=None, ignore_unknown_args=False):
     parser = _add_inference_args(parser)
     parser = _add_transformer_engine_args(parser)
     parser = _add_retro_args(parser)
+    parser = _add_msamp_args(parser)
 
     # Custom arguments.
     if extra_args_provider is not None:
@@ -1306,3 +1307,10 @@ def _add_vision_args(parser):
                        help='warmup teacher temperaure epochs')
 
     return parser
+
+
+def _add_msamp_args(parser):
+    group = parser.add_argument_group(title="msamp")
+    group.add_argument('--msamp', action='store_true', default=False,
+                       help='whether to enable msamp')
+    return parser
\ No newline at end of file
diff --git a/megatron/core/tensor_parallel/layers.py b/megatron/core/tensor_parallel/layers.py
index a86444cc..600f49d8 100644
--- a/megatron/core/tensor_parallel/layers.py
+++ b/megatron/core/tensor_parallel/layers.py
@@ -439,7 +439,9 @@ def linear_with_grad_accumulation_and_async_allreduce(
                     "maximum speedup"
                 )
                 linear_with_grad_accumulation_and_async_allreduce.warned = True
-
+    if hasattr(weight, '_scaling_metas'):
+        from msamp.megatron import FP8LinearWithGradAccumulationAndAsyncCommunication
+        return FP8LinearWithGradAccumulationAndAsyncCommunication.apply(*args)
     return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)
 
 
@@ -513,14 +515,14 @@ class ColumnParallelLinear(torch.nn.Module):
         # Initialize weight.
         if not skip_weight_param_allocation:
             if config.use_cpu_initialization:
-                self.weight = Parameter(
+                _weight = Parameter(
                     torch.empty(
                         self.output_size_per_partition, self.input_size, dtype=config.params_dtype
                     )
                 )
                 if config.perform_initialization:
                     self.master_weight = _initialize_affine_weight_cpu(
-                        self.weight,
+                        _weight,
                         self.output_size,
                         self.input_size,
                         self.output_size_per_partition,
@@ -530,7 +532,7 @@ class ColumnParallelLinear(torch.nn.Module):
                         return_master_weight=keep_master_weight_for_test,
                     )
             else:
-                self.weight = Parameter(
+                _weight = Parameter(
                     torch.empty(
                         self.output_size_per_partition,
                         self.input_size,
@@ -540,10 +542,10 @@ class ColumnParallelLinear(torch.nn.Module):
                 )
                 if config.perform_initialization:
                     _initialize_affine_weight_gpu(
-                        self.weight, init_method, partition_dim=0, stride=stride
+                        _weight, init_method, partition_dim=0, stride=stride
                     )
         else:
-            self.weight = None
+            _weight = None
 
         if bias:
             if config.use_cpu_initialization:
@@ -597,6 +599,17 @@ class ColumnParallelLinear(torch.nn.Module):
             )
 
         self._forward_impl = linear_with_grad_accumulation_and_async_allreduce
+        self.linear = torch.nn.Linear(self.input_size, self.output_size_per_partition, bias=False, dtype=config.params_dtype)
+        assert self.linear.weight.shape == _weight.shape
+        self.linear.weight = _weight
+
+    @property
+    def weight(self):
+        return self.linear.weight
+
+    @weight.setter
+    def weight(self, value):
+        raise RuntimeError('Do not set weight.')
 
     def forward(self, input_: torch.Tensor, weight: Optional[torch.Tensor] = None):
         """Forward of ColumnParallelLinear
@@ -722,14 +735,14 @@ class RowParallelLinear(torch.nn.Module):
         # we allocate the transpose.
         # Initialize weight.
         if config.use_cpu_initialization:
-            self.weight = Parameter(
+            _weight = Parameter(
                 torch.empty(
                     self.output_size, self.input_size_per_partition, dtype=config.params_dtype
                 )
             )
             if config.perform_initialization:
                 self.master_weight = _initialize_affine_weight_cpu(
-                    self.weight,
+                    _weight,
                     self.output_size,
                     self.input_size,
                     self.input_size_per_partition,
@@ -740,7 +753,7 @@ class RowParallelLinear(torch.nn.Module):
                     params_dtype=config.params_dtype,
                 )
         else:
-            self.weight = Parameter(
+            _weight = Parameter(
                 torch.empty(
                     self.output_size,
                     self.input_size_per_partition,
@@ -750,7 +763,7 @@ class RowParallelLinear(torch.nn.Module):
             )
             if config.perform_initialization:
                 _initialize_affine_weight_gpu(
-                    self.weight, init_method, partition_dim=1, stride=stride
+                    _weight, init_method, partition_dim=1, stride=stride
                 )
         if bias:
             if config.use_cpu_initialization:
@@ -774,6 +787,18 @@ class RowParallelLinear(torch.nn.Module):
 
         self._forward_impl = linear_with_grad_accumulation_and_async_allreduce
 
+        self.linear = torch.nn.Linear(self.input_size_per_partition, self.output_size, bias=False, dtype=config.params_dtype)
+        assert self.linear.weight.shape == _weight.shape
+        self.linear.weight = _weight
+
+    @property
+    def weight(self):
+        return self.linear.weight
+
+    @weight.setter
+    def weight(self, value):
+        raise RuntimeError('Do not set weight.')
+
     def forward(self, input_):
         """Forward of RowParallelLinear
 
diff --git a/megatron/model/transformer.py b/megatron/model/transformer.py
index 7aca206c..1368434a 100644
--- a/megatron/model/transformer.py
+++ b/megatron/model/transformer.py
@@ -1418,8 +1418,8 @@ class ParallelTransformer(MegatronModule):
                     tp_group=mpu.get_tensor_model_parallel_group(),
                     get_rng_state_tracker=tensor_parallel.get_cuda_rng_tracker,
                     fuse_wgrad_accumulation=config.gradient_accumulation_fusion,
-                    apply_query_key_layer_scaling=config.apply_query_key_layer_scaling,
-                    attention_softmax_in_fp32=config.attention_softmax_in_fp32,
+                    # apply_query_key_layer_scaling=config.apply_query_key_layer_scaling,
+                    # attention_softmax_in_fp32=config.attention_softmax_in_fp32,
                     seq_length=args.seq_length,
                     micro_batch_size=args.micro_batch_size,
                     sequence_parallel=config.sequence_parallel,
diff --git a/megatron/optimizer/__init__.py b/megatron/optimizer/__init__.py
index 484e9b32..e85984d7 100644
--- a/megatron/optimizer/__init__.py
+++ b/megatron/optimizer/__init__.py
@@ -5,10 +5,12 @@ from apex.optimizers import FusedSGD as SGD
 
 from megatron import get_args
 
-from .distrib_optimizer import DistributedOptimizer
 from .grad_scaler import ConstantGradScaler, DynamicGradScaler
 from .optimizer import Float16OptimizerWithFloat16Params, FP32Optimizer
 
+import torch
+from msamp.optim import LBAdamW
+from msamp.megatron import FP8DistributedOptimizer as DistributedOptimizer
 
 def get_param_groups(modules,
                      no_weight_decay_cond,
@@ -73,11 +75,21 @@ def get_megatron_optimizer(model,
                                     lr_mult)
 
     if args.optimizer == 'adam':
-        optimizer = Adam(param_groups,
-                         lr=args.lr,
-                         weight_decay=args.weight_decay,
-                         betas=(args.adam_beta1, args.adam_beta2),
-                         eps=args.adam_eps)
+        if args.msamp:
+            exp_avg_dtype, exp_avg_sq_dtype = torch.uint8, torch.float16
+            optimizer = LBAdamW(param_groups,
+                            lr=args.lr,
+                            weight_decay=args.weight_decay,
+                            betas=(args.adam_beta1, args.adam_beta2),
+                            eps=args.adam_eps,
+                            exp_avg_dtype=exp_avg_dtype, exp_avg_sq_dtype=exp_avg_sq_dtype,
+                            tensor_scale=True)
+        else:
+            optimizer = Adam(param_groups,
+                            lr=args.lr,
+                            weight_decay=args.weight_decay,
+                            betas=(args.adam_beta1, args.adam_beta2),
+                            eps=args.adam_eps)
     elif args.optimizer == 'sgd':
         optimizer = SGD(param_groups,
                         lr=args.lr,
diff --git a/megatron/optimizer/optimizer.py b/megatron/optimizer/optimizer.py
index da9cd70f..414fd887 100644
--- a/megatron/optimizer/optimizer.py
+++ b/megatron/optimizer/optimizer.py
@@ -13,13 +13,15 @@ from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
 from megatron import get_timers
 from megatron import print_rank_0
 from megatron.core import mpu, tensor_parallel
-from megatron.model import DistributedDataParallel as LocalDDP
+# from megatron.model import DistributedDataParallel as LocalDDP
 from megatron.model import Float16Module
 from megatron.model.module import param_is_not_shared
 from megatron.utils import unwrap_model
 
-from .clip_grads import clip_grad_norm_fp32, count_zeros_fp32
+from .clip_grads import count_zeros_fp32
 
+from msamp.megatron import clip_grad_norm_fp32
+from msamp.megatron import FP8DistributedDataParallel as LocalDDP
 
 def _zero_grad_group_helper(group, set_to_none):
     """Zero out the gradient for a group of parameters.
diff --git a/megatron/training.py b/megatron/training.py
index b821ae7b..99a7fadb 100644
--- a/megatron/training.py
+++ b/megatron/training.py
@@ -33,7 +33,7 @@ from megatron.initialize import initialize_megatron
 from megatron.initialize import write_args_to_tensorboard
 from megatron.initialize import set_jit_fusion_options
 from megatron.optimizer_param_scheduler import OptimizerParamScheduler
-from megatron.model import DistributedDataParallel as LocalDDP
+# from megatron.model import DistributedDataParallel as LocalDDP
 from megatron.utils import check_adlr_autoresume_termination
 from megatron.utils import unwrap_model
 from megatron.data.data_samplers import build_pretraining_data_loader
@@ -42,6 +42,10 @@ from megatron.core.pipeline_parallel import get_forward_backward_func
 from megatron.utils import report_memory
 from megatron.model.vision.knn_monitor import compute_feature_bank
 
+from msamp.nn import LinearReplacer
+from msamp.common.dtype import Dtypes
+from msamp.nn.state import model_state
+from msamp.megatron import FP8DistributedDataParallel as LocalDDP
 
 def print_datetime(string):
     """Note that this call will sync across all ranks."""
@@ -216,6 +220,9 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
     args = get_args()
     args.model_type = model_type
 
+    if args.msamp and args.transformer_impl == 'transformer_engine':
+        import msamp.te
+
     # Build model.
     if mpu.get_pipeline_model_parallel_world_size() > 1 and \
        args.virtual_pipeline_model_parallel_size is not None:
@@ -296,6 +303,20 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
     if args.fp16 or args.bf16:
         model = [Float16Module(model_module, args) for model_module in model]
 
+    if args.msamp:
+        print_rank_0("msamp is enabled")
+        model_state.use_fp8_ddp = True
+        for i in range(len(model)):
+            if args.transformer_impl == 'transformer_engine':
+                from msamp.te import TeReplacer
+                model[i] = TeReplacer.replace(model[i])
+            else:
+                model[i] = LinearReplacer.replace(model[i], Dtypes.kfloat16,
+                                                  src_rank=mpu.get_data_parallel_src_rank(),
+                                                  group=mpu.get_data_parallel_group())
+
+            print_rank_0(model[i])
+
     if wrap_with_ddp:
         if args.DDP_impl == 'torch':
             i = torch.cuda.current_device()
@@ -629,6 +650,22 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
     if iteration % args.log_interval == 0:
         elapsed_time = timers('interval-time').elapsed(barrier=True)
         elapsed_time_per_iteration = elapsed_time / total_iterations
+
+        # Compute throughput.
+        samples_per_sec = batch_size / elapsed_time_per_iteration
+
+        # Compute tflops.
+        seq_len = args.seq_length
+        hidden_size = args.hidden_size
+        num_layers = args.num_layers
+        vocab_size = args.padded_vocab_size
+
+        checkpoint_activations_factor = 4 if args.recompute_granularity else 3
+        print_rank_last(f'checkpoint_activations_factor: {checkpoint_activations_factor}')
+        coefficient = 24
+        flops_per_iteration = (coefficient * checkpoint_activations_factor * batch_size * seq_len * num_layers * (hidden_size**2)) * (1. + (seq_len / (6. * hidden_size)) + (vocab_size / (16. * num_layers * hidden_size)))
+        tflops = flops_per_iteration / (elapsed_time_per_iteration * args.world_size * (10**12))
+
         if writer:
             if args.log_timers_to_tensorboard:
                 writer.add_scalar('iteration-time',
@@ -660,6 +697,10 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
             total_loss_dict[skipped_iters_key])
         log_string += ' number of nan iterations: {:3d} |'.format(
             total_loss_dict[nan_iters_key])
+
+        log_string += ' samples per second: {:.3f} |'.format(samples_per_sec)
+        log_string += ' TFLOPs: {:.2f} |'.format(tflops)
+
         total_loss_dict[advanced_iters_key] = 0
         total_loss_dict[skipped_iters_key] = 0
         total_loss_dict[nan_iters_key] = 0
