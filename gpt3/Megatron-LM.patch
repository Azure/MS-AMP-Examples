diff --git a/megatron/arguments.py b/megatron/arguments.py
index ae42b83e..5c4b615f 100644
--- a/megatron/arguments.py
+++ b/megatron/arguments.py
@@ -38,6 +38,7 @@ def parse_args(extra_args_provider=None, ignore_unknown_args=False):
     parser = _add_inference_args(parser)
     parser = _add_transformer_engine_args(parser)
     parser = _add_retro_args(parser)
+    parser = _add_msamp_args(parser)
 
     # Custom arguments.
     if extra_args_provider is not None:
@@ -1306,3 +1307,17 @@ def _add_vision_args(parser):
                        help='warmup teacher temperaure epochs')
 
     return parser
+
+
+def _add_msamp_args(parser):
+    group = parser.add_argument_group(title="msamp")
+    group.add_argument('--msamp', action='store_true', default=False,
+                       help='whether to enable msamp')
+    # Auto scaling factor tuning for FP8 collective communication
+    group.add_argument('--wgrad-auto-scaling', action='store_true', default=False,
+                       help='whether to enable auto scaling factor tuning on weight gradients reduction')
+    group.add_argument('--wgrad-auto-scaling-ratio', type=float, default=1e-5,
+                       help='the threshold of overflow ratio for auto scaling factor tuning on weight gradients reduction')
+    group.add_argument('--wgrad-auto-scaling-window', type=int, default=1000,
+                       help='the window size for auto scaling factor tuning on weight gradients reduction')
+    return parser
diff --git a/megatron/checkpointing.py b/megatron/checkpointing.py
index e88b5851..320f2b24 100644
--- a/megatron/checkpointing.py
+++ b/megatron/checkpointing.py
@@ -106,6 +106,59 @@ def get_checkpoint_name(checkpoints_path, iteration, release=False,
 
     return os.path.join(common_path, "model_optim_rng.pt")
 
+def get_checkpoint_names(checkpoints_path, iteration,
+                         no_load_optim, use_distributed_optimizer,
+                         release=False,
+                         pipeline_parallel=None,
+                         tensor_rank=None,
+                         pipeline_rank=None):
+    """Determine the directory name for this rank's checkpoint."""
+    if release:
+        directory = 'release'
+    else:
+        directory = 'iter_{:07d}'.format(iteration)
+
+    # Use both the tensor and pipeline MP rank.
+    if pipeline_parallel is None:
+        pipeline_parallel = (mpu.get_pipeline_model_parallel_world_size() > 1)
+    if tensor_rank is None:
+        tensor_rank = mpu.get_tensor_model_parallel_rank()
+    if pipeline_rank is None:
+        pipeline_rank = mpu.get_pipeline_model_parallel_rank()
+
+    # Use both the tensor and pipeline MP rank. If using the distributed
+    # optimizer, then the optimizer's path must additionally include the
+    # data parallel rank.
+    if not pipeline_parallel:
+        common_path = os.path.join(checkpoints_path, directory,
+                            f'mp_rank_{tensor_rank:02d}')
+    else:
+        common_path = os.path.join(checkpoints_path, directory,
+                        f'mp_rank_{tensor_rank:02d}_{pipeline_rank:03d}')
+
+    # Set model/optimizer names based on use of the distributed optimizer. We
+    # additionally handle the special case where a model is pretrained using
+    # the standard optimizer, and finetuned using the distributed optimizer
+    # that's used without loading the pretraining optimizer.
+    unified_name = os.path.join(common_path, "model_optim_rng.pt")
+    distrib_model_name = os.path.join(common_path, "model_rng.pt")
+    if os.path.exists(unified_name) or not use_distributed_optimizer:
+        assert not os.path.exists(distrib_model_name)
+        if use_distributed_optimizer:
+            assert no_load_optim
+        model_name = optim_name = unified_name
+    elif os.path.exists(distrib_model_name) or use_distributed_optimizer:
+        assert use_distributed_optimizer
+        assert not os.path.exists(unified_name)
+        distrib_optim_name = os.path.join(
+            common_path + "_%03d" % mpu.get_data_parallel_rank(),
+            "optim.pt")
+        model_name = distrib_model_name
+        optim_name = distrib_optim_name
+    else:
+        raise Exception("Handle case of unified exists (%d), distrib exists (%d), and use_distributed_optimizer (%d)." % (os.path.exists(unified_name), os.path.exists(distrib_model_name), use_distributed_optimizer))
+
+    return model_name, optim_name
 
 def get_distributed_optimizer_checkpoint_name(model_checkpoint_name):
     return os.path.join(os.path.dirname(model_checkpoint_name),
@@ -122,17 +175,17 @@ def find_checkpoint_rank_0(checkpoints_path, iteration, release=False):
     """
 
     # Look for checkpoint with no pipelining
-    filename = get_checkpoint_name(checkpoints_path, iteration, release,
+    filename = get_checkpoint_names(checkpoints_path, iteration, release,
                                    pipeline_parallel=False,
                                    tensor_rank=0, pipeline_rank=0)
-    if os.path.isfile(filename):
+    if os.path.isfile(filename[0]):
         return filename
 
     # Look for checkpoint with pipelining
     filename = get_checkpoint_name(checkpoints_path, iteration, release,
                                    pipeline_parallel=True,
                                    tensor_rank=0, pipeline_rank=0)
-    if os.path.isfile(filename):
+    if os.path.isfile(filename[0]):
         return filename
 
     return None, None
@@ -225,59 +278,72 @@ def save_checkpoint(iteration, model, optimizer, opt_param_scheduler):
     # Collect rng state across data parallel ranks.
     rng_state = get_rng_state()
 
-    # Checkpoint name.
-    checkpoint_name = get_checkpoint_name(args.save, iteration)
-
-    # Save distributed optimizer's custom parameter state.
-    if args.use_distributed_optimizer:
-        optim_checkpoint_name = \
-            get_distributed_optimizer_checkpoint_name(checkpoint_name)
-        ensure_directory_exists(optim_checkpoint_name)
-        optimizer.save_parameter_state(optim_checkpoint_name)
+    # Checkpoint file names.
+    model_checkpoint_name, optim_checkpoint_name = \
+        get_checkpoint_names(args.save, iteration,
+                             args.no_load_optim, args.use_distributed_optimizer)
 
     # Collect args, model, RNG.
+    model_state_dict = {}
     if not torch.distributed.is_initialized() \
        or mpu.get_data_parallel_rank() == 0:
 
         # Arguments, iteration, and model.
-        state_dict = {}
-        state_dict['args'] = args
-        state_dict['checkpoint_version'] = 3.0
-        state_dict['iteration'] = iteration
+        model_state_dict['args'] = args
+        model_state_dict['checkpoint_version'] = 3.0
+        model_state_dict['iteration'] = iteration
         if len(model) == 1:
-            state_dict['model'] = model[0].state_dict_for_save_checkpoint()
+            model_state_dict['model'] = model[0].state_dict_for_save_checkpoint()
         else:
             for i in range(len(model)):
                 mpu.set_virtual_pipeline_model_parallel_rank(i)
-                state_dict['model%d' % i] = \
+                model_state_dict['model%d' % i] = \
                     model[i].state_dict_for_save_checkpoint()
 
-        # Optimizer stuff.
-        if not args.no_save_optim:
-            if optimizer is not None:
-                state_dict['optimizer'] = optimizer.state_dict()
-            if opt_param_scheduler is not None:
-                state_dict['opt_param_scheduler'] = \
-                    opt_param_scheduler.state_dict()
-
         # RNG states.
         if not args.no_save_rng:
-            state_dict["rng_state"] = rng_state
+            model_state_dict["rng_state"] = rng_state
 
-        # Save.
-        ensure_directory_exists(checkpoint_name)
-        torch.save(state_dict, checkpoint_name)
+    # Collect optimizer state. (Optimizer is saved separately from the model, due
+    # to the conflicting data pattern when using the distributed optimizer.)
+    optim_state_dict = {}
+    if not args.no_save_optim \
+       and (not torch.distributed.is_initialized()
+            or mpu.get_data_parallel_rank() == 0
+            or args.use_distributed_optimizer):
+
+        # Optimizer stuff.
+        if optimizer is not None:
+            optim_state_dict['optimizer'] = optimizer.state_dict()
+        if opt_param_scheduler is not None:
+            optim_state_dict['opt_param_scheduler'] = \
+                opt_param_scheduler.state_dict()
+
+    # Save.
+    if args.use_distributed_optimizer:
+        # Save model separate from optimizer.
+        if model_state_dict:
+            ensure_directory_exists(model_checkpoint_name)
+            torch.save(model_state_dict, model_checkpoint_name)
+        if optim_state_dict:
+            ensure_directory_exists(optim_checkpoint_name)
+            torch.save(optim_state_dict, optim_checkpoint_name)
+    else:
+        # Save model and optimizer together.
+        state_dict = {**model_state_dict, **optim_state_dict}
+        if state_dict: # only saves if populated (i.e., inherits conditions above)
+            ensure_directory_exists(model_checkpoint_name)
+            torch.save(state_dict, model_checkpoint_name)
 
     # Wait so everyone is done (necessary)
     if torch.distributed.is_initialized():
         torch.distributed.barrier()
 
-    print_rank_0('  successfully saved checkpoint at iteration {:7d} to {}' \
-                 .format(iteration, args.save))
+    print_rank_0('  successfully saved checkpoint at iteration {:7d} to {}'.format(
+        iteration, args.save))
 
     # And update the latest iteration
-    if not torch.distributed.is_initialized() \
-       or torch.distributed.get_rank() == 0:
+    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:
         tracker_filename = get_checkpoint_tracker_filename(args.save)
         with open(tracker_filename, 'w') as f:
             f.write(str(iteration))
@@ -381,17 +447,26 @@ def _load_base_checkpoint(load_dir, rank0=False):
 
     # Checkpoint.
     if rank0:
-        checkpoint_name = find_checkpoint_rank_0(load_dir, iteration, release)
+        checkpoint_names = find_checkpoint_rank_0(load_dir, iteration, release)
     else:
-        checkpoint_name = get_checkpoint_name(load_dir, iteration, release)
+        args = get_args()
+        checkpoint_names = get_checkpoint_names(load_dir, iteration,
+                                                  args.no_load_optim,
+                                                  args.use_distributed_optimizer, release)
         if release:
             print_rank_0(f' loading release checkpoint from {load_dir}')
         else:
             print_rank_0(f' loading checkpoint from {load_dir} at iteration {iteration}')
 
+    model_checkpoint_name, optim_checkpoint_name = checkpoint_names
+
     # Load the checkpoint.
     try:
-        state_dict = torch.load(checkpoint_name, map_location='cpu')
+        model_state_dict = torch.load(model_checkpoint_name, map_location='cpu')
+        if args.use_distributed_optimizer:
+            optim_state_dict = torch.load(optim_checkpoint_name, map_location='cpu')
+        else:
+            optim_state_dict = model_state_dict
     except ModuleNotFoundError:
         from megatron.fp16_deprecated import loss_scaler
         # For backward compatibility.
@@ -401,7 +476,6 @@ def _load_base_checkpoint(load_dir, rank0=False):
             'megatron.fp16_deprecated.loss_scaler']
         sys.modules['megatron.fp16.loss_scaler'] = sys.modules[
             'megatron.fp16_deprecated.loss_scaler']
-        state_dict = torch.load(checkpoint_name, map_location='cpu')
         sys.modules.pop('fp16.loss_scaler', None)
         sys.modules.pop('megatron.fp16.loss_scaler', None)
     except BaseException as e:
@@ -409,7 +483,7 @@ def _load_base_checkpoint(load_dir, rank0=False):
         print_rank_0(e)
         sys.exit()
 
-    return state_dict, checkpoint_name, release
+    return model_state_dict, optim_state_dict, release
 
 
 def load_args_from_checkpoint(args, load_arg='load'):
@@ -504,10 +578,14 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
 
     model = unwrap_model(model)
 
-    state_dict, checkpoint_name, release = _load_base_checkpoint(load_dir, rank0=False)
+    model_state_dict, optim_state_dict, release = \
+        _load_base_checkpoint(load_dir,
+                              #no_load_optim=args.no_load_optim,
+                              #use_distributed_optimizer=args.use_distributed_optimizer,
+                              rank0=False)
 
     # Checkpoint not loaded.
-    if state_dict is None:
+    if model_state_dict is None:
 
         # Conditionally exit at this point.
         if args.exit_on_missing_checkpoint:
@@ -518,18 +596,18 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
         # Iteration defaults to 0.
         return 0
 
-    # Set checkpoint version.
-    set_checkpoint_version(state_dict.get('checkpoint_version', 0))
+    # set checkpoint version
+    set_checkpoint_version(model_state_dict.get('checkpoint_version', 0))
 
     # Set iteration.
     if args.finetune or release:
         iteration = 0
     else:
         try:
-            iteration = state_dict['iteration']
+            iteration = model_state_dict['iteration']
         except KeyError:
             try:  # Backward compatible with older checkpoints
-                iteration = state_dict['total_iters']
+                iteration = model_state_dict['total_iters']
             except KeyError:
                 print_rank_0('A metadata file exists but unable to load '
                              'iteration from checkpoint {}, exiting'.format(
@@ -539,8 +617,8 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
     # Check arguments.
     assert args.consumed_train_samples == 0
     assert args.consumed_valid_samples == 0
-    if 'args' in state_dict and not args.finetune:
-        checkpoint_args = state_dict['args']
+    if 'args' in model_state_dict and not args.finetune:
+        checkpoint_args = model_state_dict['args']
         check_checkpoint_args(checkpoint_args)
         args.consumed_train_samples = getattr(checkpoint_args,
                                               'consumed_train_samples', 0)
@@ -552,13 +630,13 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
 
     # Model.
     if len(model) == 1:
-        model[0].load_state_dict(state_dict['model'], strict=strict)
+        model[0].load_state_dict(model_state_dict['model'], strict=strict)
     else:
         for i in range(len(model)):
             mpu.set_virtual_pipeline_model_parallel_rank(i)
-            model[i].load_state_dict(state_dict['model%d' % i], strict=strict)
+            model[i].load_state_dict(model_state_dict['model%d' % i], strict=strict)
 
-    # Fix up query/key/value matrix ordering if needed.
+    # Fix up query/key/value matrix ordering if needed
     checkpoint_version = get_checkpoint_version()
     print_rank_0(f' checkpoint version {checkpoint_version}')
     fix_query_key_value_ordering(model, checkpoint_version)
@@ -566,27 +644,13 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
     # Optimizer.
     if not release and not args.finetune and not args.no_load_optim:
         try:
-            # Load state dict.
             if optimizer is not None:
-                optimizer.load_state_dict(state_dict['optimizer'])
-
-            # Load distributed optimizer's custom parameter state.
-            if args.use_distributed_optimizer:
-                tracker_filename = get_checkpoint_tracker_filename(load_dir)
-                iteration, release = read_metadata(tracker_filename)
-                model_checkpoint_name = \
-                    get_checkpoint_name(load_dir, iteration, release)
-                optim_checkpoint_name = \
-                    get_distributed_optimizer_checkpoint_name(
-                        model_checkpoint_name)
-                optimizer.load_parameter_state(optim_checkpoint_name)
-
-            # Load scheduler.
+                optimizer.load_state_dict(optim_state_dict['optimizer'])
             if opt_param_scheduler is not None:
-                if 'lr_scheduler' in state_dict: # backward compatbility
-                    opt_param_scheduler.load_state_dict(state_dict['lr_scheduler'])
+                if 'lr_scheduler' in optim_state_dict: # backward compatbility
+                    opt_param_scheduler.load_state_dict(optim_state_dict['lr_scheduler'])
                 else:
-                    opt_param_scheduler.load_state_dict(state_dict['opt_param_scheduler'])
+                    opt_param_scheduler.load_state_dict(optim_state_dict['opt_param_scheduler'])
         except KeyError:
             print_rank_0('Unable to load optimizer from checkpoint {}. '
                          'Specify --no-load-optim or --finetune to prevent '
@@ -600,13 +664,13 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
     # rng states.
     if not release and not args.finetune and not args.no_load_rng:
         try:
-            if 'rng_state' in state_dict:
+            if 'rng_state' in model_state_dict:
                 # access rng_state for data parallel rank
                 if args.data_parallel_random_init:
 
-                    rng_state = state_dict['rng_state'][mpu.get_data_parallel_rank()]
+                    rng_state = model_state_dict['rng_state'][mpu.get_data_parallel_rank()]
                 else:
-                    rng_state = state_dict['rng_state'][0]
+                    rng_state = model_state_dict['rng_state'][0]
                 random.setstate(rng_state['random_rng_state'])
                 np.random.set_state(rng_state['np_rng_state'])
                 torch.set_rng_state(rng_state['torch_rng_state'])
@@ -617,15 +681,15 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
                 tensor_parallel.get_cuda_rng_tracker().set_states(
                     rng_state['rng_tracker_states'])
             else:  # backward compatability
-                random.setstate(state_dict['random_rng_state'])
-                np.random.set_state(state_dict['np_rng_state'])
-                torch.set_rng_state(state_dict['torch_rng_state'])
-                torch.cuda.set_rng_state(state_dict['cuda_rng_state'])
+                random.setstate(model_state_dict['random_rng_state'])
+                np.random.set_state(model_state_dict['np_rng_state'])
+                torch.set_rng_state(model_state_dict['torch_rng_state'])
+                torch.cuda.set_rng_state(model_state_dict['cuda_rng_state'])
                 # Check for empty states array
-                if not state_dict['rng_tracker_states']:
+                if not model_state_dict['rng_tracker_states']:
                     raise KeyError
                 tensor_parallel.get_cuda_rng_tracker().set_states(
-                    state_dict['rng_tracker_states'])
+                    model_state_dict['rng_tracker_states'])
         except KeyError:
             print_rank_0('Unable to load rng state from checkpoint {}. '
                          'Specify --no-load-rng or --finetune to prevent '
@@ -642,7 +706,6 @@ def load_checkpoint(model, optimizer, opt_param_scheduler, load_arg='load', stri
 
     return iteration
 
-
 def load_biencoder_checkpoint(model, only_query_model=False,
                               only_context_model=False, custom_load_path=None):
     """
diff --git a/megatron/core/tensor_parallel/layers.py b/megatron/core/tensor_parallel/layers.py
index a86444cc..600f49d8 100644
--- a/megatron/core/tensor_parallel/layers.py
+++ b/megatron/core/tensor_parallel/layers.py
@@ -439,7 +439,9 @@ def linear_with_grad_accumulation_and_async_allreduce(
                     "maximum speedup"
                 )
                 linear_with_grad_accumulation_and_async_allreduce.warned = True
-
+    if hasattr(weight, '_scaling_metas'):
+        from msamp.megatron import FP8LinearWithGradAccumulationAndAsyncCommunication
+        return FP8LinearWithGradAccumulationAndAsyncCommunication.apply(*args)
     return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)
 
 
@@ -513,14 +515,14 @@ class ColumnParallelLinear(torch.nn.Module):
         # Initialize weight.
         if not skip_weight_param_allocation:
             if config.use_cpu_initialization:
-                self.weight = Parameter(
+                _weight = Parameter(
                     torch.empty(
                         self.output_size_per_partition, self.input_size, dtype=config.params_dtype
                     )
                 )
                 if config.perform_initialization:
                     self.master_weight = _initialize_affine_weight_cpu(
-                        self.weight,
+                        _weight,
                         self.output_size,
                         self.input_size,
                         self.output_size_per_partition,
@@ -530,7 +532,7 @@ class ColumnParallelLinear(torch.nn.Module):
                         return_master_weight=keep_master_weight_for_test,
                     )
             else:
-                self.weight = Parameter(
+                _weight = Parameter(
                     torch.empty(
                         self.output_size_per_partition,
                         self.input_size,
@@ -540,10 +542,10 @@ class ColumnParallelLinear(torch.nn.Module):
                 )
                 if config.perform_initialization:
                     _initialize_affine_weight_gpu(
-                        self.weight, init_method, partition_dim=0, stride=stride
+                        _weight, init_method, partition_dim=0, stride=stride
                     )
         else:
-            self.weight = None
+            _weight = None
 
         if bias:
             if config.use_cpu_initialization:
@@ -597,6 +599,17 @@ class ColumnParallelLinear(torch.nn.Module):
             )
 
         self._forward_impl = linear_with_grad_accumulation_and_async_allreduce
+        self.linear = torch.nn.Linear(self.input_size, self.output_size_per_partition, bias=False, dtype=config.params_dtype)
+        assert self.linear.weight.shape == _weight.shape
+        self.linear.weight = _weight
+
+    @property
+    def weight(self):
+        return self.linear.weight
+
+    @weight.setter
+    def weight(self, value):
+        raise RuntimeError('Do not set weight.')
 
     def forward(self, input_: torch.Tensor, weight: Optional[torch.Tensor] = None):
         """Forward of ColumnParallelLinear
@@ -722,14 +735,14 @@ class RowParallelLinear(torch.nn.Module):
         # we allocate the transpose.
         # Initialize weight.
         if config.use_cpu_initialization:
-            self.weight = Parameter(
+            _weight = Parameter(
                 torch.empty(
                     self.output_size, self.input_size_per_partition, dtype=config.params_dtype
                 )
             )
             if config.perform_initialization:
                 self.master_weight = _initialize_affine_weight_cpu(
-                    self.weight,
+                    _weight,
                     self.output_size,
                     self.input_size,
                     self.input_size_per_partition,
@@ -740,7 +753,7 @@ class RowParallelLinear(torch.nn.Module):
                     params_dtype=config.params_dtype,
                 )
         else:
-            self.weight = Parameter(
+            _weight = Parameter(
                 torch.empty(
                     self.output_size,
                     self.input_size_per_partition,
@@ -750,7 +763,7 @@ class RowParallelLinear(torch.nn.Module):
             )
             if config.perform_initialization:
                 _initialize_affine_weight_gpu(
-                    self.weight, init_method, partition_dim=1, stride=stride
+                    _weight, init_method, partition_dim=1, stride=stride
                 )
         if bias:
             if config.use_cpu_initialization:
@@ -774,6 +787,18 @@ class RowParallelLinear(torch.nn.Module):
 
         self._forward_impl = linear_with_grad_accumulation_and_async_allreduce
 
+        self.linear = torch.nn.Linear(self.input_size_per_partition, self.output_size, bias=False, dtype=config.params_dtype)
+        assert self.linear.weight.shape == _weight.shape
+        self.linear.weight = _weight
+
+    @property
+    def weight(self):
+        return self.linear.weight
+
+    @weight.setter
+    def weight(self, value):
+        raise RuntimeError('Do not set weight.')
+
     def forward(self, input_):
         """Forward of RowParallelLinear
 
diff --git a/megatron/optimizer/__init__.py b/megatron/optimizer/__init__.py
index 484e9b32..e85984d7 100644
--- a/megatron/optimizer/__init__.py
+++ b/megatron/optimizer/__init__.py
@@ -5,10 +5,12 @@ from apex.optimizers import FusedSGD as SGD
 
 from megatron import get_args
 
-from .distrib_optimizer import DistributedOptimizer
 from .grad_scaler import ConstantGradScaler, DynamicGradScaler
 from .optimizer import Float16OptimizerWithFloat16Params, FP32Optimizer
 
+import torch
+from msamp.optim import LBAdamW
+from msamp.megatron import FP8DistributedOptimizer as DistributedOptimizer
 
 def get_param_groups(modules,
                      no_weight_decay_cond,
@@ -73,11 +75,21 @@ def get_megatron_optimizer(model,
                                     lr_mult)
 
     if args.optimizer == 'adam':
-        optimizer = Adam(param_groups,
-                         lr=args.lr,
-                         weight_decay=args.weight_decay,
-                         betas=(args.adam_beta1, args.adam_beta2),
-                         eps=args.adam_eps)
+        if args.msamp:
+            exp_avg_dtype, exp_avg_sq_dtype = torch.uint8, torch.float16
+            optimizer = LBAdamW(param_groups,
+                            lr=args.lr,
+                            weight_decay=args.weight_decay,
+                            betas=(args.adam_beta1, args.adam_beta2),
+                            eps=args.adam_eps,
+                            exp_avg_dtype=exp_avg_dtype, exp_avg_sq_dtype=exp_avg_sq_dtype,
+                            tensor_scale=True)
+        else:
+            optimizer = Adam(param_groups,
+                            lr=args.lr,
+                            weight_decay=args.weight_decay,
+                            betas=(args.adam_beta1, args.adam_beta2),
+                            eps=args.adam_eps)
     elif args.optimizer == 'sgd':
         optimizer = SGD(param_groups,
                         lr=args.lr,
diff --git a/megatron/optimizer/optimizer.py b/megatron/optimizer/optimizer.py
index da9cd70f..414fd887 100644
--- a/megatron/optimizer/optimizer.py
+++ b/megatron/optimizer/optimizer.py
@@ -13,13 +13,15 @@ from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
 from megatron import get_timers
 from megatron import print_rank_0
 from megatron.core import mpu, tensor_parallel
-from megatron.model import DistributedDataParallel as LocalDDP
+# from megatron.model import DistributedDataParallel as LocalDDP
 from megatron.model import Float16Module
 from megatron.model.module import param_is_not_shared
 from megatron.utils import unwrap_model
 
-from .clip_grads import clip_grad_norm_fp32, count_zeros_fp32
+from .clip_grads import count_zeros_fp32
 
+from msamp.megatron import clip_grad_norm_fp32
+from msamp.megatron import FP8DistributedDataParallel as LocalDDP
 
 def _zero_grad_group_helper(group, set_to_none):
     """Zero out the gradient for a group of parameters.
diff --git a/megatron/training.py b/megatron/training.py
index b821ae7b..99a7fadb 100644
--- a/megatron/training.py
+++ b/megatron/training.py
@@ -33,7 +33,7 @@ from megatron.initialize import initialize_megatron
 from megatron.initialize import write_args_to_tensorboard
 from megatron.initialize import set_jit_fusion_options
 from megatron.optimizer_param_scheduler import OptimizerParamScheduler
-from megatron.model import DistributedDataParallel as LocalDDP
+# from megatron.model import DistributedDataParallel as LocalDDP
 from megatron.utils import check_adlr_autoresume_termination
 from megatron.utils import unwrap_model
 from megatron.data.data_samplers import build_pretraining_data_loader
@@ -42,6 +42,10 @@ from megatron.core.pipeline_parallel import get_forward_backward_func
 from megatron.utils import report_memory
 from megatron.model.vision.knn_monitor import compute_feature_bank
 
+from msamp.nn import LinearReplacer
+from msamp.common.dtype import Dtypes
+from msamp.nn.state import model_state
+from msamp.megatron import FP8DistributedDataParallel as LocalDDP
 
 def print_datetime(string):
     """Note that this call will sync across all ranks."""
@@ -216,6 +220,9 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
     args = get_args()
     args.model_type = model_type
 
+    if args.msamp and args.transformer_impl == 'transformer_engine':
+        import msamp.te
+
     # Build model.
     if mpu.get_pipeline_model_parallel_world_size() > 1 and \
        args.virtual_pipeline_model_parallel_size is not None:
@@ -296,6 +303,20 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap
     if args.fp16 or args.bf16:
         model = [Float16Module(model_module, args) for model_module in model]
 
+    if args.msamp:
+        print_rank_0("msamp is enabled")
+        model_state.use_fp8_ddp = True
+        for i in range(len(model)):
+            if args.transformer_impl == 'transformer_engine':
+                from msamp.te import TeReplacer
+                model[i] = TeReplacer.replace(model[i])
+            else:
+                model[i] = LinearReplacer.replace(model[i], Dtypes.kfloat16,
+                                                  src_rank=mpu.get_data_parallel_src_rank(),
+                                                  group=mpu.get_data_parallel_group())
+
+            print_rank_0(model[i])
+
     if wrap_with_ddp:
         if args.DDP_impl == 'torch':
             i = torch.cuda.current_device()
@@ -629,6 +650,22 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
     if iteration % args.log_interval == 0:
         elapsed_time = timers('interval-time').elapsed(barrier=True)
         elapsed_time_per_iteration = elapsed_time / total_iterations
+
+        # Compute throughput.
+        samples_per_sec = batch_size / elapsed_time_per_iteration
+
+        # Compute tflops.
+        seq_len = args.seq_length
+        hidden_size = args.hidden_size
+        num_layers = args.num_layers
+        vocab_size = args.padded_vocab_size
+
+        checkpoint_activations_factor = 4 if args.recompute_granularity else 3
+        print_rank_last(f'checkpoint_activations_factor: {checkpoint_activations_factor}')
+        coefficient = 24
+        flops_per_iteration = (coefficient * checkpoint_activations_factor * batch_size * seq_len * num_layers * (hidden_size**2)) * (1. + (seq_len / (6. * hidden_size)) + (vocab_size / (16. * num_layers * hidden_size)))
+        tflops = flops_per_iteration / (elapsed_time_per_iteration * args.world_size * (10**12))
+
         if writer:
             if args.log_timers_to_tensorboard:
                 writer.add_scalar('iteration-time',
@@ -660,6 +697,10 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
             total_loss_dict[skipped_iters_key])
         log_string += ' number of nan iterations: {:3d} |'.format(
             total_loss_dict[nan_iters_key])
+
+        log_string += ' samples per second: {:.3f} |'.format(samples_per_sec)
+        log_string += ' TFLOPs: {:.2f} |'.format(tflops)
+
         total_loss_dict[advanced_iters_key] = 0
         total_loss_dict[skipped_iters_key] = 0
         total_loss_dict[nan_iters_key] = 0
